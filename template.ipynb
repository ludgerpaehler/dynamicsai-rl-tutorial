{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/nzolman/dynamicsai-rl-tutorial/blob/main/template.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: ray[rllib]\n"
     ]
    }
   ],
   "source": [
    "# Only validated for Python 3.10!\n",
    "!pip install torch ray[rllib] gymnasium numpy matplotlib tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from gymnasium import Env\n",
    "from gymnasium.spaces import Box\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating an Environment and Defining the Objective\n",
    "Here, we create a simple environment to interact with. We'll be examing the single inverted pendulum given by the following equations:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\dot{\\theta} & = \\omega \\\\\n",
    "    \\dot{\\omega} &= \\frac{g}{L} \\sin(\\theta) + u\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Where $u$ is the control input into the system and the unstable equilibrium is at $(x,y) = (0, 1)$ and defined to be when $\\theta =0$.\n",
    "\n",
    "[Gymnasium documentation](https://gymnasium.farama.org/content/basic_usage/#/tutorials/environment_creation) \\\n",
    "[Gymnasium Github](https://github.com/Farama-Foundation/Gymnasium/tree/main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PendEnv(Env):\n",
    "    ''' \n",
    "    Pendulum environment with dynamics\n",
    "    theta'(t) = omega\n",
    "    omega'(t) = -(g/L) sin(theta) + u\n",
    "    \n",
    "    Objective: control pendulum to be in the upright position (theta=0)\n",
    "    '''\n",
    "    def __init__(self, config=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        <...>\n",
    "        <...>\n",
    "\n",
    "        \n",
    "    def reset(self, seed=None, options=None):\n",
    "        '''Reset the environment'''\n",
    "        \n",
    "        <...>\n",
    "        <...>\n",
    "        <...>\n",
    "        \n",
    "        return self.state, info\n",
    "    \n",
    "    def get_reward(self, obs, action):\n",
    "        '''Obtain Reward'''\n",
    "        \n",
    "        <...>\n",
    "        \n",
    "        return <...>\n",
    "        \n",
    "    def step(self, action):\n",
    "        '''\n",
    "        Update environment by executing action\n",
    "        '''\n",
    "        \n",
    "        <...>\n",
    "        \n",
    "        return self.state, reward, terminated, truncated, info \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Env Sanity Check\n",
    "Now that we've coded up an environment, let's check that it works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<...>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Action\n",
    "Great, the functions at least seem to be outputting what they're supposed to. Let's step through the environment with no action for lots of steps and make sure this looks like it's working properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<...>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(observation_list[:,0], label = 'x')\n",
    "plt.plot(observation_list[:,1], label = 'y')\n",
    "plt.plot(theta_list,label = r'$\\theta$' )\n",
    "plt.plot(omega_list, label = r'$\\omega$')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Action\n",
    "Ok, great! That seems to meet expectations. We start with $\\theta \\approx 0$, it swings down past the discontinuity at $\\theta = \\pi$ and approaches $\\theta \\approx 0$ again. \n",
    "\n",
    "For one final sanity check, let's incorporate random sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<...>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(observation_list[:,0], label = 'x')\n",
    "plt.plot(observation_list[:,1], label = 'y')\n",
    "plt.plot(theta_list,label = r'$\\theta$' )\n",
    "plt.plot(omega_list, label = r'$\\omega$')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RLLib Check\n",
    "One final thing that we can do is appeal to RLlib, which we'll use in just a little bit, to check that our environment meets all the necessary API standards. \n",
    "\n",
    "(In practice, this is really useful because RLlib and Gymnasium have both undergone different API changes recently.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<...>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training an Agent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RLlib\n",
    "We'll be using RLlib (\"RL Library\") because it's gaining a lot of popularity, has multi-agent RL support, and has _many_ implementations for different DRL algorithms. However, most importantly, RLlib makes it very easy transition between developing locally on your laptop and scaling up to multi-node HPC clusters or cloud infrastructures without you having to change your code very much. \n",
    "\n",
    "We'll be toying around with their implementation of \"Proximal Policy Optimization\" (PPO), which is a type of policy gradient algorithm. All you have to know for the purposes of this tutorial is that the output of the policy network is a continuous action distribution, $\\pi(x)$. And we sample the distribution to get a control value: $u \\sim \\pi(x)$. \n",
    "\n",
    "In particular, this particular implementation parameterizes a diagonal gaussian distribution: $\\pi(x) = N(\\mu(x), \\sigma(x))$, so the output is the policy network is the mean and standard deviation. \n",
    "\n",
    "[RLlib Documentation](https://docs.ray.io/en/latest/rllib/index.html)\n",
    "\n",
    "[Proximal Policy Optimization (PPO)](https://docs.ray.io/en/latest/rllib/rllib-algorithms.html#ppo)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Config\n",
    "RLlib is \"config-driven\", and there are a ton of ways to customize your training environment, and even tune your hyperparameters. But we'll just be scratching the surface ever-so-lightly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<...>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<...>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we've done it! We've trained! You can officially say that you've trained a Deep Reinforcement Learning policy. The catch is that we've only done this for a single training iteration. But what does this mean? Let's inspect this results dictionary, `res`, that we've created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<...>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, `num_env_steps_trained=4000`, meaning that we've trained on 4000 steps. Since we set $dt=0.01$, this means that it's about 40 seconds worth of data. We could write a loop and train for many more iterations, but let's inspect this first!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Our first Trained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<...>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(theta_list, label = r'$\\theta$' )\n",
    "plt.plot(omega_list, label = r'$\\omega$')\n",
    "plt.plot(action_list, label = 'u')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yeah, we didn't do so hot, huh? We can train longer, but let's dig a bit deeper before that. We can actually access the policy directly"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Analysis: Value Function and Policy\n",
    "Because this is low-dimensional, we can actually poke at this a bit more than we normally can. Let's make a mesh of the state-space and look at both the policy function (i.e. the control) and the value function (i.e. \"how much reward do we expect to get in a certain state\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<...>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<...>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "fig, axes = plt.subplots(1,3, figsize=(15,5))\n",
    "\n",
    "# This lets us define where the colorbars should go\n",
    "dividers = [make_axes_locatable(ax) for ax in axes]\n",
    "cax_list = [divider.append_axes('top', size='5%', pad=0.25) for divider in dividers]  \n",
    "\n",
    "# Create main plots\n",
    "im_0 = axes[0].scatter(theta_mesh, omega_mesh, s = 10, cmap = plt.cm.RdBu,\n",
    "                       c =control_mean)\n",
    "\n",
    "im_1 = axes[1].scatter(theta_mesh, omega_mesh, s = 10, cmap = plt.cm.RdBu,\n",
    "                       c =control_std)\n",
    "\n",
    "im_2 = axes[2].scatter(theta_mesh, omega_mesh, s = 10, cmap = plt.cm.RdBu,\n",
    "                       c =vf_vals)\n",
    "\n",
    "ims = [im_0, im_1, im_2]\n",
    "\n",
    "# create colorbars\n",
    "cb_list = []\n",
    "for im, cax in zip(ims, cax_list):\n",
    "    cb= fig.colorbar(im, cax=cax, orientation='horizontal')\n",
    "    cb_list.append(cb)  \n",
    "\n",
    "cb_list[0].set_label('Action Mean', labelpad=-50)\n",
    "cb_list[1].set_label('Action Std', labelpad=-50)\n",
    "cb_list[2].set_label('Value Function', labelpad=-50)  \n",
    "\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xlabel(r'$\\theta$')\n",
    "    ax.set_ylabel(r'$\\omega$')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok! We've got some interesting structure going on here! Note that we don't actually have that much variance in the functions. The policy is approximately zero mean with unit width. The value function, however, has a bit more rich structure, although it isn't entirely clear what it means yet. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train for more iterations!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Before we train longer, let's wrap up a couple of these functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data(env, algo, seed=0, n_steps = 1000):\n",
    "    \n",
    "    <...>\n",
    "    \n",
    "    return observation_list, action_list, rew_list\n",
    "\n",
    "\n",
    "def plot_meshes(policy, mesh, plot_log=False):\n",
    "    \n",
    "    <...>\n",
    "    \n",
    "    return fig, axes, cb_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "Now let's train for 10 iterations or so. We'll evaluate our performance on the same initial condition after every training iteration. Something extra we'll do here is keep a copy of the policy states so we can access them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "env = PendEnv()\n",
    "observations = []\n",
    "actions = []\n",
    "rews = []\n",
    "policy_list = []\n",
    "n_train_iterations = 10\n",
    "\n",
    "for i in tqdm(range(n_train_iterations)):\n",
    "    \n",
    "    # train the agent for one iteration\n",
    "    <...>\n",
    "    \n",
    "    # keep a copy of the policy\n",
    "    <...>\n",
    "    \n",
    "    # collect data from the current agent\n",
    "    <...>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing the Training\n",
    "First thing's first, we want to figure out if our performance has improved! Let's plot the total reward per iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<...>\n",
    "\n",
    "plt.plot(tot_rews)\n",
    "\n",
    "plt.title('Total Reward Progress')\n",
    "plt.xlabel('Training Iteration')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That seems promising! Let's plot the rolled out trajectories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = plt.cm.tab10.colors\n",
    "\n",
    "<...>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! We can see that, indeed, near the equilibrium we can stablize this pendulum! Let's plot the policy and value functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<...>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# You're an RL Practitioner Now!\n",
    "Play around with the code! Some motivating problems from the pendulum:\n",
    "- What happens if we train even longer?\n",
    "- What if we start from an initial condition in one of these \"uncertain\" regions? \n",
    "- What happens if you change to Sparse Rewards? (+1 if we're less than a maximum angle, -1 otherwise?)\n",
    "- Robustness: \n",
    "    - What if we add noise to the observations? \n",
    "    - What if we change the model parameters? (e.g. $L =3$) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
